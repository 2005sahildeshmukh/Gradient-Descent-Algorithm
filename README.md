# Gradient-Descent-Algorith

This repository contains an implementation and explanation of the Gradient Descent optimization algorithm, a fundamental technique used in machine learning and deep learning to minimize cost functions and optimize model parameters.

🔍 Overview

Gradient Descent is an iterative optimization algorithm that updates model parameters in the direction of the negative gradient of the cost function until convergence. It is the backbone of training models such as:
Linear Regression
Logistic Regression
Neural Networks

📘 Features
Implementation of Batch, Stochastic, and Mini-Batch Gradient Descent.
Step-by-step explanation with mathematical formulas.
Example on regression data for demonstration.
Well-commented and beginner-friendly code.

🧮 Formula
The update rule for parameters is:

θ:=θ−α⋅∇J(θ)

Where:

θ: Model parameters (weights, biases, etc.)

α: Learning rate

∇J(θ): Gradient of the cost function

🚀 Applications
Training ML models (Linear/Logistic Regression)
Deep learning optimization (Neural Networks)
General optimization problems in AI/ML
